<h2>
    Publications
</h2>
<p>
Please see my <a href="https://www.semanticscholar.org/author/Doyoung-Kim/2180527259">Semantic Scholar</a> or
    <a href="https://scholar.google.com/citations?user=PJR9ogMAAAAJ">Google Scholar</a> profiles for the full list.
</p>
<i>* denotes equal contribution.</i>
<div class="panel panel-default display: inline-block">
    <div class="panel-heading"><h3 class="panel-title">2024</h3></div>
    <ul class="list-group">
        <li class="list-group-item">
            <p id="cosup">
                <b>Beyond Next Token Prediction: semiparametric token sequence cosupervision</b><br>
                Hyunji Lee*, <u>Doyoung Kim*</u>, Jihoon Jun, Sejune Joo, Joel Jang, Kyoung-Woon On, Minjoon Seo<br>
            </p>
            [<a href="">paper</a>]
        </li>     
        <li class="list-group-item">
            <p id="ground">
                <b>
                    How Well Do Large Language Models Truly Ground?</b><br>
                Hyunji Lee, Se June Joo, Chaeeun Kim, Joel Jang, <u>Doyoung Kim</u>, Kyoung-Woon On, Minjoon Seo<br>
                NAACL 2024<br>
            </p>
            [<a href="https://arxiv.org/abs/2311.09069">paper</a>]
        </li>
        <li class="list-group-item">
            <p id="flask">
                <b>FLASK: Fine-grained Language Model Evaluation based on Alignment Skill Sets</b>
                <span class="label label-primary">spotlight</span><br>
                Seonghyeon Ye*, <u>Doyoung Kim*</u>, Sungdong Kim, Hyeonbin Hwang, Seungone Kim, Yongrae Jo, James Thorne, Juho Kim, Minjoon Seo<br>
                ICLR 2024<br>
            </p>
            [<a href="https://arxiv.org/abs/2307.10928">paper</a>]
        </li>       

    </ul>
</div>

<div class="panel panel-default">
    <div class="panel-heading"><h3 class="panel-title">2023</h3></div>
    <ul class="list-group">
        <li class="list-group-item">
            <p id="cot">
                <b>
The CoT Collection: Improving Zero-shot and Few-shot Learning of Language Models via Chain-of-Thought Fine-Tuning</b><br>
                Seungone Kim, Se June Joo, <u>Doyoung Kim</u>, Joel Jang, Seonghyeon Ye, Jamin Shin, Minjoon Seo<br>
                EMNLP 2023<br>
                [<a href="https://arxiv.org/abs/2305.14045">paper</a>]
            </p>
        </li>
        <li class="list-group-item">
            <p id="rospr">
                <b>Efficiently Enhancing Zero-Shot Performance of Instruction Following Model via Retrieval of Soft Prompt</b><br>
                Seonghyeon Ye, Joel Jang, <u>Doyoung Kim</u>, Yongrae Jo, Minjoon Seo<br>
                EMNLP 2023 Findings<br>
                [<a href="https://arxiv.org/abs/2210.03029">paper</a>]
            </p>
        </li>
        <li class="list-group-item">
            <p id="expert-lm">
                <b>Exploring the Benefits of Training Expert Language Models over Instruction Tuning</b><br>
                Joel Jang, Seungone Kim, Seonghyeon Ye, <u>Doyoung Kim</u>, Lajanugen Logeswaran, Moontae Lee, Kyungjae Lee, Minjoon Seo<br>
                ICML 2023<br>
                [<a href="https://arxiv.org/abs/2302.03202">paper</a>]
            </p>
        </li>

        <li class="list-group-item">
            <p id="flipped-learning">
                <b>Guess the Instruction! Flipped Learning Makes Language Models Stronger Zero-Shot Learners</b><br>
                Seonghyeon Ye, <u>Doyoung Kim</u>, Joel Jang, Joongbo Shin, Minjoon Seo<br>
                ICLR 2023<br>
                [<a href="https://arxiv.org/abs/2210.02969">paper</a>]
            </p>
        </li>
    </ul>
</div>

